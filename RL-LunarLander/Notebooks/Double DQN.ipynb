{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Double DQN\n",
    "\n",
    "Although DQN manages to alleviate the memory required by Q learning, DQN does not manage to solve some of the other flaws of Q learning. For instance in DQN, Q-value that is the expected value is calculated with the reward added to the next state's maximum Q value as seen from the bellman equation. As a result if for a certain state, the Q value calculated is high then the value that is obtained from the output of the neural network for that particular state will get higher everytime. This leads the algorithm to be overly optimistic in taking that action even though it does not actually provide that much value. For example if for a particular episode action A does well and receives a high reward the neural network will learn to give action A a high approximation even though other actions might be more valuable in some cases.\n",
    "\n",
    "Double DQN manages to solve this problem by using two identical neural network models. One learns during the experience replay just like DQN does, and the other one is a copy of the last episode of the former model. If the model overestimates the Q value for a particular episode the idea is that the model from the previous episode will control the bias of the model when updating the Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    def __init__(self,maxSize,stateDim):\n",
    "        self.state=np.zeros((maxSize,stateDim))\n",
    "        self.action=np.zeros(maxSize,dtype= np.int8)\n",
    "        self.reward=np.zeros(maxSize)\n",
    "        self.done=np.zeros(maxSize,)\n",
    "        self.nextState=np.zeros((maxSize,stateDim))\n",
    "        self.maxSize=maxSize\n",
    "        self.curser=0\n",
    "        self.size=0\n",
    "\n",
    "    def save(self,state,action,reward,nextState,done):\n",
    "        self.state[self.curser]=state\n",
    "        self.action[self.curser]=action\n",
    "        self.reward[self.curser]=reward\n",
    "        self.nextState[self.curser]=nextState\n",
    "        self.done[self.curser]=done\n",
    "        self.curser=(self.curser+1)%self.maxSize\n",
    "        if self.size<self.maxSize:\n",
    "            self.size+=1 \n",
    "\n",
    "    def sample(self,batchSize):\n",
    "        batchSize=min(self.size,batchSize-1)\n",
    "        indexes=np.random.choice([i for i in range(self.size-1)],batchSize)\n",
    "        return self.state[indexes],self.action[indexes],self.reward[indexes],self.nextState[indexes],self.done[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,stateShape,actionShape,exploreRate,exploreRateDecay,minimumExploreRate,gamma,copyNetsCycle):\n",
    "        self.gamma=gamma\n",
    "        self.exploreRate=exploreRate\n",
    "        self.exploreRateDecay=exploreRateDecay\n",
    "        self.minimumExploreRate=minimumExploreRate\n",
    "        self.actionShape=actionShape\n",
    "        self.memory=replayBuffer(1000000,stateShape)\n",
    "        self.model=self.buildModel(stateShape,actionShape)\n",
    "        self.model.compile(optimizer='Adam',loss='mse')\n",
    "        self.tModel=self.buildModel(stateShape,actionShape)\n",
    "        self.tModel.compile(optimizer='Adam',loss='mse')\n",
    "        self.learnThreshold=0\n",
    "        self.copyNetsCycle=copyNetsCycle\n",
    "\n",
    "    def buildModel(self,input,output):\n",
    "        inputLayer=keras.Input(shape=(input,))\n",
    "        layer=Dense(256,activation='relu')(inputLayer)\n",
    "        layer=Dense(256,activation='relu')(layer)\n",
    "        outputLayer=Dense(output)(layer)\n",
    "        model=keras.Model(inputs=inputLayer,outputs=outputLayer)\n",
    "        model.compile(optimizer='Adam',loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def getAction(self,state):\n",
    "        q=self.model.predict(np.expand_dims(state,axis=0), verbose = 0)[0]\n",
    "        if np.random.random()<=self.exploreRate:\n",
    "            return np.random.choice([i for i in range(env.action_space.n)])\n",
    "        else:\n",
    "            return np.argmax(q)\n",
    "\n",
    "    def exploreDecay(self):\n",
    "        self.exploreRate=max(self.exploreRate*self.exploreRateDecay,self.minimumExploreRate)\n",
    "\n",
    "    def saveModel(self,modelName=\"DoubleDQN_LunarLanderV2.h\"):\n",
    "        self.model.save_weights(f\"{modelName}\")\n",
    "\n",
    "    def loadModel(self,modelName=\"DoubleDQN_LunarLanderV2.h\"):\n",
    "        self.model.load_weights(f\"{modelName}\")\n",
    "        self.tModel.set_weights(self.model.get_weights())\n",
    "      \n",
    "    def learn(self,batchSize=64):\n",
    "        if self.memory.size>batchSize:\n",
    "            states,actions,rewards,nextStates,done=self.memory.sample(batchSize)\n",
    "            qState=self.model.predict(states,verbose = 0)\n",
    "            qNextState=self.model.predict(nextStates,verbose = 0)\n",
    "            qNextStateTarget=self.tModel.predict(nextStates, verbose = 0)\n",
    "            maxActions=np.argmax(qNextState,axis=1)\n",
    "            batchIndex = np.arange(batchSize-1, dtype=np.int32)\n",
    "            qState[batchIndex,actions]=(rewards+(self.gamma*qNextStateTarget[batchIndex,maxActions.astype(int)]*(1-done)))\n",
    "            _=self.model.fit(x=states,y=qState,verbose=0)\n",
    "            self.learnThreshold+=1\n",
    "\n",
    "            if(self.learnThreshold%self.copyNetsCycle)==0:\n",
    "                self.tModel.set_weights(self.model.get_weights())\n",
    "                self.saveModel()\n",
    "                self.learnThreshold=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1   reward: -168.27615399262075  avg so far:-168.27615399262075 exploreRate:0.977\n",
      "episode: 2   reward: -89.76465335118822  avg so far:-129.0204036719045 exploreRate:0.954529\n",
      "episode: 3   reward: -77.9104763052607  avg so far:-111.98376121635657 exploreRate:0.932574833\n",
      "episode: 4   reward: -120.96418987150106  avg so far:-114.22886838014269 exploreRate:0.9111256118409999\n",
      "episode: 5   reward: -153.8418495798761  avg so far:-122.15146462008938 exploreRate:0.8901697227686569\n",
      "episode: 6   reward: -176.85571967658854  avg so far:-131.26884046283922 exploreRate:0.8696958191449777\n",
      "episode: 7   reward: -130.66869539257755  avg so far:-131.18310545280184 exploreRate:0.8496928153046432\n",
      "episode: 8   reward: -63.529832939746775  avg so far:-122.72644638866996 exploreRate:0.8301498805526364\n",
      "episode: 9   reward: -77.66841331054219  avg so far:-117.71999826887799 exploreRate:0.8110564332999257\n",
      "episode: 10   reward: -213.1426998597127  avg so far:-127.26226842796146 exploreRate:0.7924021353340274\n",
      "episode: 11   reward: -158.4307591944218  avg so far:-130.09576758854877 exploreRate:0.7741768862213447\n",
      "episode: 12   reward: -213.3313329977507  avg so far:-137.03206470598226 exploreRate:0.7563708178382538\n",
      "episode: 13   reward: -137.29487329672475  avg so far:-137.05228075142398 exploreRate:0.7389742890279739\n",
      "episode: 14   reward: -98.91112418848275  avg so far:-134.32791242549962 exploreRate:0.7219778803803305\n",
      "episode: 15   reward: -103.05891936751156  avg so far:-132.2433128883004 exploreRate:0.7053723891315828\n",
      "episode: 16   reward: -120.11567144260249  avg so far:-131.4853352979443 exploreRate:0.6891488241815564\n",
      "episode: 17   reward: -227.90605968436722  avg so far:-137.15714261479272 exploreRate:0.6732984012253806\n",
      "episode: 18   reward: -222.43213520681292  avg so far:-141.89464220323828 exploreRate:0.6578125379971969\n",
      "episode: 19   reward: -24.97078424147749  avg so far:-135.74075494209296 exploreRate:0.6426828496232613\n",
      "episode: 20   reward: -180.80756953484172  avg so far:-137.99409567173038 exploreRate:0.6279011440819262\n",
      "episode: 21   reward: -71.66311589705253  avg so far:-134.83547758722193 exploreRate:0.6134594177680419\n",
      "episode: 22   reward: -30.794979132059964  avg so far:-130.10636402107818 exploreRate:0.5993498511593769\n",
      "episode: 23   reward: -43.82331989728365  avg so far:-126.35492732004363 exploreRate:0.5855648045827112\n",
      "episode: 24   reward: -46.46628552846067  avg so far:-123.02623391206104 exploreRate:0.5720968140773088\n",
      "episode: 25   reward: -97.71785040147333  avg so far:-122.01389857163753 exploreRate:0.5589385873535307\n",
      "episode: 26   reward: -59.2825237105447  avg so far:-119.60115338467243 exploreRate:0.5460829998443996\n",
      "episode: 27   reward: 30.507196888396408  avg so far:-114.04158485604025 exploreRate:0.5335230908479783\n",
      "episode: 28   reward: -127.04213442646541  avg so far:-114.50589019784115 exploreRate:0.5212520597584748\n",
      "episode: 29   reward: 1.2770748558238267  avg so far:-110.51337416150788 exploreRate:0.5092632623840299\n",
      "episode: 30   reward: 7.31810379046334  avg so far:-106.58565822977552 exploreRate:0.49755020734919714\n",
      "episode: 31   reward: -23.26767385046803  avg so far:-103.89798131431398 exploreRate:0.4861065525801656\n",
      "episode: 32   reward: -261.04226931062675  avg so far:-108.80874031419873 exploreRate:0.4749261018708218\n",
      "episode: 33   reward: 0.9315623092770977  avg so far:-105.48327659833582 exploreRate:0.4640028015277929\n",
      "episode: 34   reward: 1.1066186822901187  avg so far:-102.34827967831743 exploreRate:0.45333073709265365\n",
      "episode: 35   reward: -44.57601059196012  avg so far:-100.69764341870722 exploreRate:0.4429041301395226\n",
      "episode: 36   reward: 6.5257565626751415  avg so far:-97.7192156414466 exploreRate:0.4327173351463136\n",
      "episode: 37   reward: -51.25985889920939  avg so far:-96.46355735111587 exploreRate:0.4227648364379484\n",
      "episode: 38   reward: -175.65695330474932  avg so far:-98.54759408673779 exploreRate:0.41304124519987556\n",
      "episode: 39   reward: -44.52509415623469  avg so far:-97.16240178082747 exploreRate:0.40354129656027843\n",
      "episode: 40   reward: -10.992889705228094  avg so far:-95.00816397893747 exploreRate:0.394259846739392\n",
      "episode: 41   reward: -9.584477868923273  avg so far:-92.92465943966884 exploreRate:0.385191870264386\n",
      "episode: 42   reward: -43.658739460855486  avg so far:-91.75166134493519 exploreRate:0.3763324572483051\n",
      "episode: 43   reward: -128.33370212380453  avg so far:-92.6024064793275 exploreRate:0.3676768107315941\n",
      "episode: 44   reward: 3.3424828583413273  avg so far:-90.42184081256231 exploreRate:0.35922024408476744\n",
      "episode: 45   reward: 13.185402557690301  avg so far:-88.11945762655668 exploreRate:0.3509581784708178\n",
      "episode: 46   reward: -83.09295514296412  avg so far:-88.0101858334351 exploreRate:0.34288614036598897\n",
      "episode: 47   reward: -30.294314739895402  avg so far:-86.78218857612575 exploreRate:0.33499975913757124\n",
      "episode: 48   reward: 65.74778818737383  avg so far:-83.60448072688617 exploreRate:0.3272947646774071\n",
      "episode: 49   reward: 114.9299050351966  avg so far:-79.55275856847632 exploreRate:0.3197669850898267\n",
      "episode: 50   reward: -35.07267539480645  avg so far:-78.66315690500292 exploreRate:0.31241234443276067\n",
      "episode: 51   reward: 2.5847384162271823  avg so far:-75.24593905682596 exploreRate:0.30522686051080716\n",
      "episode: 52   reward: -8.432759281272567  avg so far:-73.61930117542765 exploreRate:0.2982066427190586\n",
      "episode: 53   reward: -123.08375899759949  avg so far:-74.52276682927442 exploreRate:0.2913478899365203\n",
      "episode: 54   reward: -3.1885238656344272  avg so far:-72.16725350915709 exploreRate:0.2846468884679803\n",
      "episode: 55   reward: 16.274898296065373  avg so far:-68.76491855163826 exploreRate:0.27810001003321677\n",
      "episode: 56   reward: 32.519882618832554  avg so far:-64.57740650572984 exploreRate:0.2717037098024528\n",
      "episode: 57   reward: -76.76831940083572  avg so far:-63.499398985895006 exploreRate:0.26545452447699636\n",
      "episode: 58   reward: -67.45448516866585  avg so far:-63.577892030473386 exploreRate:0.25934907041402544\n",
      "episode: 59   reward: 41.81932986830688  avg so far:-61.188137166896404 exploreRate:0.25338404179450286\n",
      "episode: 60   reward: -253.04539371005632  avg so far:-61.98619104390328 exploreRate:0.2475562088332293\n",
      "episode: 61   reward: -2.935896184322242  avg so far:-58.876293783701286 exploreRate:0.241862416030065\n",
      "episode: 62   reward: -345.87824805571734  avg so far:-61.527232084860614 exploreRate:0.2362995804613735\n",
      "episode: 63   reward: 29.456483836719798  avg so far:-58.19220494219173 exploreRate:0.2308646901107619\n",
      "episode: 64   reward: 25.542741717347567  avg so far:-55.703127624075115 exploreRate:0.2255548022382144\n",
      "episode: 65   reward: -353.09476102370456  avg so far:-60.703844457198976 exploreRate:0.22036704178673547\n",
      "episode: 66   reward: 89.38404865727304  avg so far:-56.513850055201466 exploreRate:0.21529859982564054\n",
      "episode: 67   reward: -137.21403148032  avg so far:-54.700009491120525 exploreRate:0.2103467320296508\n",
      "episode: 68   reward: 73.10024123155057  avg so far:-48.78936196235326 exploreRate:0.20550875719296882\n",
      "episode: 69   reward: -377.44739338706955  avg so far:-55.838894145265094 exploreRate:0.20078205577753053\n",
      "episode: 70   reward: 8.7212990222243  avg so far:-52.048316774123776 exploreRate:0.19616406849464732\n",
      "episode: 71   reward: -168.14168638172927  avg so far:-53.97788818381731 exploreRate:0.19165229491927044\n",
      "episode: 72   reward: 11.153313970688174  avg so far:-53.13892232176235 exploreRate:0.18724429213612723\n",
      "episode: 73   reward: 211.3904649824808  avg so far:-48.03464662416705 exploreRate:0.1829376734169963\n",
      "episode: 74   reward: 133.80662389230855  avg so far:-44.429188435751676 exploreRate:0.17873010692840538\n",
      "episode: 75   reward: 220.02888669206618  avg so far:-38.07425369388088 exploreRate:0.17461931446905204\n",
      "episode: 76   reward: 99.37442349888097  avg so far:-34.90111474969236 exploreRate:0.17060307023626384\n",
      "episode: 77   reward: 82.83070258449914  avg so far:-33.85464463577031 exploreRate:0.16667919962082978\n",
      "episode: 78   reward: 213.29665114217778  avg so far:-27.047868924397445 exploreRate:0.16284557802955069\n",
      "episode: 79   reward: -209.02672751527743  avg so far:-31.253944971819475 exploreRate:0.159100129734871\n",
      "episode: 80   reward: 86.49832127981152  avg so far:-29.67034062203251 exploreRate:0.15544082675096896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 81   reward: -31.53557601427778  avg so far:-29.835698665308705 exploreRate:0.15186568773569667\n",
      "episode: 82   reward: 31.47342310783995  avg so far:-23.98538481693937 exploreRate:0.14837277691777565\n",
      "episode: 83   reward: -59.592635160419704  avg so far:-25.1958687663333 exploreRate:0.1449602030486668\n",
      "episode: 84   reward: -292.71908844083646  avg so far:-31.072382908795834 exploreRate:0.14162611837854747\n",
      "episode: 85   reward: 107.17814113966773  avg so far:-28.03729987416328 exploreRate:0.13836871765584088\n",
      "episode: 86   reward: -271.8392521447573  avg so far:-33.60460004831193 exploreRate:0.13518623714975653\n",
      "episode: 87   reward: 183.96497983091388  avg so far:-28.900103273709465 exploreRate:0.13207695369531214\n",
      "episode: 88   reward: -32.44175346268007  avg so far:-26.035799276868083 exploreRate:0.12903918376031995\n",
      "episode: 89   reward: -50.4423761879724  avg so far:-26.154144917502837 exploreRate:0.1260712825338326\n",
      "episode: 90   reward: 118.2925421222753  avg so far:-23.568436280952763 exploreRate:0.12317164303555445\n",
      "episode: 91   reward: 108.7145196755374  avg so far:-21.20245633006356 exploreRate:0.12033869524573669\n",
      "episode: 92   reward: -262.75597515590175  avg so far:-25.58440104396448 exploreRate:0.11757090525508475\n",
      "episode: 93   reward: 111.38408660617345  avg so far:-20.790045269364917 exploreRate:0.11486677443421779\n",
      "episode: 94   reward: -47.37085855728361  avg so far:-21.804312097677418 exploreRate:0.11222483862223077\n",
      "episode: 95   reward: 27.123094701511533  avg so far:-21.525558254800995 exploreRate:0.10964366733391946\n",
      "episode: 96   reward: -54.87655857999968  avg so far:-20.961230323541702 exploreRate:0.1071218629852393\n",
      "episode: 97   reward: 282.1237737364378  avg so far:-14.712868554015042 exploreRate:0.1046580601365788\n",
      "episode: 98   reward: 247.0492031131491  avg so far:-11.086840255499535 exploreRate:0.10225092475343749\n",
      "episode: 99   reward: 194.4488512973935  avg so far:-9.496461330255599 exploreRate:0.09989915348410842\n",
      "episode: 100   reward: 269.73210239514503  avg so far:-3.400365774456566 exploreRate:0.09760147295397392\n",
      "episode: 101   reward: -56.566205528090215  avg so far:-4.583384653342919 exploreRate:0.09535663907603252\n",
      "episode: 102   reward: 279.4563654548591  avg so far:1.1743978413797163 exploreRate:0.09316343637728378\n",
      "episode: 103   reward: 22.0609711082585  avg so far:4.077292443496874 exploreRate:0.09102067734060625\n",
      "episode: 104   reward: -8.48116727872997  avg so far:3.971439575234964 exploreRate:0.0889272017617723\n",
      "episode: 105   reward: -206.73395327377895  avg so far:-0.48873745616192593 exploreRate:0.08688187612125153\n",
      "episode: 106   reward: 248.7288796956505  avg so far:3.835442485374435 exploreRate:0.08488359297046275\n",
      "episode: 107   reward: -161.07934424956284  avg so far:2.149221988399893 exploreRate:0.0829312703321421\n",
      "episode: 108   reward: -202.36873185540355  avg so far:-0.5490629453348629 exploreRate:0.08102385111450283\n",
      "episode: 109   reward: 246.81496673161317  avg so far:3.5508497919312623 exploreRate:0.07916030253886927\n",
      "episode: 110   reward: 262.42836836236114  avg so far:13.860325033379613 exploreRate:0.07733961558047528\n",
      "episode: 111   reward: 245.07654784257522  avg so far:18.820573913917563 exploreRate:0.07556080442212434\n",
      "episode: 112   reward: 242.54870820349635  avg so far:30.589113039101836 exploreRate:0.07382290592041549\n",
      "episode: 113   reward: 294.89247579037516  avg so far:35.897832878174945 exploreRate:0.07212497908424594\n",
      "episode: 114   reward: -0.11255600237899177  avg so far:35.38472692378041 exploreRate:0.07046610456530827\n",
      "episode: 115   reward: 295.6002556511311  avg so far:48.35862725727713 exploreRate:0.06884538416030618\n",
      "episode: 116   reward: -83.1615532052405  avg so far:44.90771522002686 exploreRate:0.06726194032461914\n",
      "episode: 117   reward: 246.44109220219286  avg so far:52.58081769367712 exploreRate:0.0657149156971529\n",
      "episode: 118   reward: 35.691921023325364  avg so far:51.832651289512626 exploreRate:0.06420347263611839\n",
      "episode: 119   reward: -9.432066600036606  avg so far:59.19295782525327 exploreRate:0.06272679276548766\n",
      "episode: 120   reward: 26.06915473048018  avg so far:59.53991493941839 exploreRate:0.06128407653188145\n",
      "episode: 121   reward: 9.373375785278924  avg so far:63.09021618275855 exploreRate:0.05987454277164817\n",
      "episode: 122   reward: -4.311540841184851  avg so far:62.78091908652109 exploreRate:0.05849742828790026\n",
      "episode: 123   reward: -55.67087030691354  avg so far:57.439692380733206 exploreRate:0.057151987437278555\n",
      "episode: 124   reward: 2.037543032458217  avg so far:54.80431076353621 exploreRate:0.05583749172622115\n",
      "episode: 125   reward: -223.43775401172826  avg so far:45.93497794946031 exploreRate:0.05455322941651806\n",
      "episode: 126   reward: -56.24473410328792  avg so far:42.822594797416926 exploreRate:0.05329850513993814\n",
      "episode: 127   reward: -212.7355123189664  avg so far:36.91127049934762 exploreRate:0.05207263952171956\n",
      "episode: 128   reward: -141.77612886524437  avg so far:29.80981489919918 exploreRate:0.05087496881272001\n",
      "episode: 129   reward: 240.81525833041698  avg so far:38.806654616113065 exploreRate:0.04970484453002745\n",
      "episode: 130   reward: 254.62320005875148  avg so far:42.169152191691865 exploreRate:0.04856163310583682\n",
      "episode: 131   reward: -210.7567129490925  avg so far:38.58472945299557 exploreRate:0.04744471554440257\n",
      "episode: 132   reward: -217.80439392705483  avg so far:33.59917311229767 exploreRate:0.04635348708688131\n",
      "episode: 133   reward: -226.97712217269714  avg so far:30.251483372052125 exploreRate:0.045287356883883044\n",
      "episode: 134   reward: 227.2654986190725  avg so far:40.65117511325031 exploreRate:0.04424574767555373\n",
      "episode: 135   reward: -11.951624426307248  avg so far:38.26857980193081 exploreRate:0.043228095479015995\n",
      "episode: 136   reward: 5.685579706032982  avg so far:43.81907643894661 exploreRate:0.04223384928299863\n",
      "episode: 137   reward: -143.46239760580994  avg so far:37.27052889021214 exploreRate:0.04126247074948966\n",
      "episode: 138   reward: 260.7903207498696  avg so far:43.13517037446313 exploreRate:0.040313433922251396\n",
      "episode: 139   reward: 5.56435520882988  avg so far:44.255305002399176 exploreRate:0.03938622494203961\n",
      "episode: 140   reward: -304.4920116244689  avg so far:35.799613927464286 exploreRate:0.0384803417683727\n",
      "episode: 141   reward: -88.98470656675822  avg so far:31.84562940261838 exploreRate:0.037595293907700125\n",
      "episode: 142   reward: 6.637063868047338  avg so far:37.23349018309736 exploreRate:0.03673060214782302\n",
      "episode: 143   reward: -20.474106405318608  avg so far:34.59632632286752 exploreRate:0.03588579829842309\n",
      "episode: 144   reward: 216.38893462467377  avg so far:39.87152218650666 exploreRate:0.035060424937559356\n",
      "episode: 145   reward: 189.53387882363802  avg so far:43.119737868949194 exploreRate:0.03425403516399549\n",
      "episode: 146   reward: 250.3780273283299  avg so far:49.22482958711579 exploreRate:0.0334661923552236\n",
      "episode: 147   reward: 275.47801868360506  avg so far:49.09191448605913 exploreRate:0.032696469931053455\n",
      "episode: 148   reward: 17.3738931129583  avg so far:44.49840828605531 exploreRate:0.031944451122639224\n"
     ]
    }
   ],
   "source": [
    "agent=Agent(stateShape=env.observation_space.shape[0],actionShape=env.action_space.n , \\\n",
    "            exploreRate=1.0,exploreRateDecay=0.977,minimumExploreRate=0.01,gamma=0.99,copyNetsCycle=100)\n",
    "\n",
    "averageRewards=[]\n",
    "totalRewards=[]\n",
    "for i in range(1,300):\n",
    "    done=False\n",
    "    state=env.reset()\n",
    "    rewards=0\n",
    "    while not done:\n",
    "        action=agent.getAction(state)\n",
    "        nextState,reward,done,info=env.step(action)\n",
    "        agent.memory.save(state,action,reward,nextState,int(done))\n",
    "        rewards+=reward\n",
    "        state=nextState\n",
    "        agent.learn(batchSize=64)\n",
    "        \n",
    "    agent.exploreDecay()\n",
    "    totalRewards.append(rewards)    \n",
    "    averageRewards.append(np.mean(totalRewards[-50:]))\n",
    "          \n",
    "    print(f\"episode: {i}   reward: {rewards}  avg so far:{averageRewards[-1]} exploreRate:{agent.exploreRate}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was not able to finish training due to the time constraint as well as the lack of a continued access to the lab PCs as home PCs cannot provide the performance and memory requirements needed. However some useful insights can still be gleaned.\n",
    "- As shown above the model was only able to train till episode 148 and achieved an average reward of 44. For DQN to achieve the same level of reward it took ~400 episodes. Hence we can see that the Double DQN improves the convergence speed tremendously by estimating the Value function more accurately.\n",
    "\n",
    "- We also see from the trend of average rewards that the model is steadily learning to estimate the Value function better and is not very volatile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "650f4021d2b10ec59d729e1d92ce703ab82c45089f90d72fd0153a7ca0efe76f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
